##线性回归

###解决问题

先上一个例子

人们去某一家银行贷款，贷款额度与工资和年龄的关系如下：

| 工资    | 年龄   | 额度    |
| ----- | ---- | ----- |
| 4000  | 25   | 20000 |
| 8000  | 30   | 70000 |
| 5000  | 28   | 35000 |
| 7500  | 33   | 50000 |
| 12000 | 40   | 85000 |

预测，下一个人去银行贷款的额度是多少？

工资和年龄对贷款额度的影响有多大？

###思路推理

工资和年龄是我们的两个**特征**，额度是我们想预测的结果，**这个结果是一个具体的数值**。

![1.png](http://upload-images.jianshu.io/upload_images/4739877-3dec58543f575a31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对于每一个样本，都存在误差，记做：

![2.png](http://upload-images.jianshu.io/upload_images/4739877-fe9ae185676e9dfc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们应该要让误差越小，那我们的参数就越好，我们的目的还是要求参数 \theta。

![gif.latex.gif](http://upload-images.jianshu.io/upload_images/4739877-40d6fcd405156881.gif?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这时，我们作出一个**假设**，所有样本数据的误差是**独立**且具有**相同分布**，服从高斯分布的。

![3.png](http://upload-images.jianshu.io/upload_images/4739877-3a1b5b3b6c9d2d18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

高斯分布的y坐标是概率值，x坐标是各个样本的误差。这里可以看出我们的假设就是误差越大的概率会比较小，大部分的误差都接近于0，这样的分布才是我们所希望的分布情况。

下面是高斯分布的公式：

![屏幕快照 2018-01-19 21.42.01.png](http://upload-images.jianshu.io/upload_images/4739877-19ef08b4f13d85b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

中间的公式就是高斯分布的公式。

这时我们想，由于误差是服从高斯分布的，是不是只有当每次误差的概率越大，那误差就越接近于0啊，也是我们想要的情况。

所以我们把所有误差的概率相乘，以便让此结果最大，这样就它的似然函数：

![屏幕快照 2018-01-19 21.46.57.png](http://upload-images.jianshu.io/upload_images/4739877-fa6eb8b525565b09.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

乘法想求最大值难解，我们转换为对数似然以便就加法最大值。

![屏幕快照 2018-01-19 21.49.51.png](http://upload-images.jianshu.io/upload_images/4739877-c9a7ed9b68fc02a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

化简过程略过若干步骤，最后上面试子，要想让最上面那个式子最大，由于它前半部分是个常数，后面是减去一个数，这样我们让最后那个数最小就行了。 就是让这个最小二乘法的式子最小。

这个式子是一个方程，在数学中，我们想求一个函数的的最低点，我们是不是需要求这个函数的偏导等于0的情况就是啊？ 是的：

![屏幕快照 2018-01-19 21.56.01.png](http://upload-images.jianshu.io/upload_images/4739877-7b7273ab92163a86.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上面的化简需要注意的是，无论X还是theta还是y，都是矩阵，需要用矩阵的算法来化简。

这样我们就求得theta的一个具体值。哇？？？ 

是的，特殊情况（线性回归）就是能求出来。

###结果

不过我们一般都不是这样去直接求得一个theta，而是用梯度下降的方法去慢慢找一个最优的theta。

## 梯度下降

当我们得到最小二乘法的目标函数之后，我们需要去求什么样的theta可以让这个函数的值是最小的。

![屏幕快照 2018-01-19 22.03.28.png](http://upload-images.jianshu.io/upload_images/4739877-a13d764ba9a0690b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

首先，我们可以随意定义一个theta矩阵，比如{1，1，1，1 … ，1} 里面的元素都是1。

然后我们求出现在的目标函数的值是多少。

然后我们更新theta的值，在次求出目标函数的值。这样两次求出的值看谁最小。

当我们更新n次之后，我们可以从这n次里面挑一个能使目标函数的值最小的theta矩阵。

### 怎么更新theta参数

我们可以先求得在原theta点上，目标函数的导数，数学中，函数在某点的导数就是，函数在这个点上，往下一个方向移动的方向。这样的话我们可以让theta往这个方向上移动一定的距离，得到theta更新后的值。

这个更新一定的距离，我们称为学习率（步长）。

导数需要我们去求，上面的函数中，导数为。

![屏幕快照 2018-01-19 22.18.17.png](http://upload-images.jianshu.io/upload_images/4739877-111b2750c81d8fc9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

学习率(步长)我们自己定义, 一般很小，不行就更小。

![CodeCogsEqn.gif](http://upload-images.jianshu.io/upload_images/4739877-5c86770ea6c57317.gif?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

alpha为学习率(步长)

### 梯度下降的常用方式

* 批量梯度下降: 就是考虑所有的样本，上式中的m为全部样本个数（这样容易得到最优解，但是样本非常多速度非常慢）
* 随机梯度下降: 就是每次就考虑1个样本，m=1（这样速度快，但是不一定每次都朝着收敛方向移动）
* 小批量梯度下降: 每次考虑一部分样本，m=10（实用）



##逻辑回归

###解决问题

逻辑回归解决的是分类问题。

另一个例子，某次考试的成绩出来了，学生们考了2个科目，每个科目的分数为x1和x2，是否通过的结果为y，y的取值为0或1。

预测，下一个人的成绩出来后，能否通过考试？

科目1和科目2对考试结果的影响有多大？

![CodeCogsEqn-2.gif](http://upload-images.jianshu.io/upload_images/4739877-db540c9fbd3868f8.gif?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

y的取值为0或者1。

###思路推理

这里我们引入Sigmoid函数

* Sigmoid函数

![屏幕快照 2018-01-19 22.44.57.png](http://upload-images.jianshu.io/upload_images/4739877-6771eaa0cd228bab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

将y带入函数有：

![屏幕快照 2018-01-19 22.54.23.png](http://upload-images.jianshu.io/upload_images/4739877-a46adaed33ba47c1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们假设(y==1时)通过考试的概率服从Sigmoid函数的分布，那么没有通过考试(y==0时)的概率就是1减去通过考试的概率。

推理有：

![屏幕快照 2018-01-19 22.55.46.png](http://upload-images.jianshu.io/upload_images/4739877-16a46fbcb01bd075.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们将式子进行整合，当y=0时，只有右边的式子；当y=1时，只有左边的式子，恰好是左边分类任务的情况。这样得一个式子可以表达前面的分类任务的两个式子，这两部分是等价的。

这样，就得到了事件发生的概率函数。

回到了概率问题，我们希望当x的取某个值时，通过和未通过的概率都越大越好（就是概率越接近100%最好），这样才最接近我们现实的情况。

这样就得到似然函数：

![屏幕快照 2018-01-19 23.46.28.png](http://upload-images.jianshu.io/upload_images/4739877-fd3db4a03b7e0cc8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

转换为梯度下降任务后求导：

![屏幕快照 2018-01-19 23.47.57.png](http://upload-images.jianshu.io/upload_images/4739877-6dc4ec8f134f510a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


###结果

这样，我们使用梯度下降的方法，先定义一个theta矩阵，

求对数似然函数变换的（损失函数）的值。

然后定义步长，更新theta矩阵，继续求损失函数的值。

从这n次迭代中挑选使损失函数最小的theta矩阵。

![屏幕快照 2018-01-19 23.53.10.png](http://upload-images.jianshu.io/upload_images/4739877-d6a9f2049d4fabd1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



###逻辑回归实践

[https://github.com/yyllove123/StudyMachineLearning](https://github.com/yyllove123/StudyMachineLearning)


